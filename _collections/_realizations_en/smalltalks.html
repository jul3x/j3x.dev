---
date: 2023-05-16
name: SmallTalks
img: smalltalks.webp
short: Prototype of&nbsp;an intelligent conversational toy

img_pos: center;
video: "https://www.youtube.com/embed/Q8EayWG7v_8"
project_url: "https://www.youtube.com/shorts/Q8EayWG7v_8"
---

<p>
    In early 2023, together with&nbsp;<a href="https://github.com/MichalBortkiewicz" rel="noreferrer" target="_blank" class="bold">@michalbortkiewicz</a>, we decided to&nbsp;delve deeper into the&nbsp;increasingly popular topic of&nbsp;large language models and&nbsp;their practical applications.
</p>

<p>
We created a&nbsp;prototype of&nbsp;an intelligent conversational toy that met the&nbsp;requirements of&nbsp;an artificial assistant. The toy could recognize speech, infer, formulate responses to&nbsp;questions, and&nbsp;produce speech. In various settings, it could also tell stories and&nbsp;even solve problems.
The prototype was based on&nbsp;a <strong>Raspberry Pi 4</strong> powered by&nbsp;a battery and&nbsp;connected to&nbsp;the internet. The software managing the&nbsp;conversation was written in&nbsp;<strong>Python</strong>. We performed text transcription (<strong>Speech-To-Text</strong>) and&nbsp;speech synthesis (<strong>Text-To-Speech</strong>) on&nbsp;the device using local open-source models (<strong>Faster Whisper</strong> and&nbsp;<strong>Tacotron 2</strong>). Inference and&nbsp;response generation were handled by&nbsp;external models (<strong>OpenAI GPT-3.5</strong>) with&nbsp;dedicated <strong>prompt engineering</strong>. The entire setup was embedded inside a&nbsp;plush teddy bear.
</p>

<p>
    The biggest challenge was dealing with&nbsp;delays. Each AI model introduced additional latency, ranging from several hundred milliseconds to&nbsp;a few seconds. To maintain smooth conversation flow in&nbsp;the early stages of&nbsp;prototype development, we used scripted filler phrases. Multithreading allowed us to&nbsp;bridge the&nbsp;gaps between user input and&nbsp;toy responses.
    </p>

<p>
    The main goal was to&nbsp;create a&nbsp;generic solution that could facilitate the&nbsp;development of&nbsp;entire series of&nbsp;interactive toys. We experimented with&nbsp;educational toys which teach languages (like the&nbsp;one shown in&nbsp;the attached video) or&nbsp;colors and&nbsp;also with&nbsp;toys with&nbsp;purely entertaining aspects. One such example was the&nbsp;interactive <strong>Yoda Master</strong>, which presented a&nbsp;unique challenge due to&nbsp;its distinct voice and&nbsp;grammar. We managed to&nbsp;train a&nbsp;<strong>Text-To-Speech</strong> model and&nbsp;craft appropriate input data for&nbsp;a large language model. This resulted in&nbsp;a prototype toy for&nbsp;<strong>Star Wars</strong> fans, which narrated the&nbsp;universe in&nbsp;the voice and&nbsp;style of&nbsp;the <strong>Master</strong>.
    </p>

<p>
    The project then transitioned from the&nbsp;prototype phase to&nbsp;the <strong>MVP</strong> phase, where we aimed to&nbsp;create a&nbsp;more practical solution â€” incorporating most computations in&nbsp;the cloud and&nbsp;a horizontally scalable architecture.
    However, the&nbsp;work was halted due to&nbsp;the market's insufficient maturity for&nbsp;the idea we presented.
    </p>
